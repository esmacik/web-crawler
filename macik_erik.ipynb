{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Erik Macik\n",
    "\n",
    "# Assignment 1: Crawling and Indexing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define useful methods\n",
    "### Recursive crawling method, robot parser method, and stop word reader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from urllib import request\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "# Crawl beginning at the given URL\n",
    "def crawl(url, stop_words):\n",
    "    print('Starting crawl at', url, '...\\n')\n",
    "\n",
    "    # Partse robots.txt\n",
    "    allow, disallow = get_parsed_robots('http://www.cs.utep.edu/makbar/A3/robots.txt')\n",
    "\n",
    "    # Open file to record results\n",
    "    file_name = 'macik_erik.txt'\n",
    "    results_file = open(file_name, 'w')\n",
    "\n",
    "    # Begin recursive crawling\n",
    "    crawl_r(url, stop_words, set(), 0, results_file, allow, disallow)\n",
    "\n",
    "    # Save file\n",
    "    results_file.close()\n",
    "    print('\\nSaved results to', file_name)\n",
    "\n",
    "# Helper method to recursively crawl websites\n",
    "def crawl_r(url, stop_words, visited, depth, results_file, allow, disallow):\n",
    "\n",
    "    # Record Progress\n",
    "    print(('\\t' * depth) + 'Crawling', url, '...')\n",
    "\n",
    "    # Read page contents\n",
    "    page = request.urlopen(url).read().decode('utf8')\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    # Create set of unique links on this page\n",
    "    links = set()\n",
    "    for link in soup.find_all('a'):\n",
    "        href_link = link.get('href')\n",
    "        links.add(link if '://' in link else urljoin(url, href_link))\n",
    "\n",
    "    # Get web page text\n",
    "    raw = soup.get_text()\n",
    "\n",
    "    # Remove punctuation from string\n",
    "    raw = raw.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Record word frequencies\n",
    "    word_counts = {word: raw.count(word) for word in raw.split() if word not in stop_words}\n",
    "\n",
    "    # Write indexing to file\n",
    "    results_file.write(('\\t' * depth) + url + ' -> ')\n",
    "    for word, count in word_counts.items():\n",
    "        results_file.write(word + '::' + str(count) + ' ')\n",
    "    results_file.write('\\n')\n",
    "\n",
    "    # Add url to visited to prevent looping\n",
    "    visited.add(url)\n",
    "\n",
    "    # Visit other links if they have not already been visited and if allowed by robots.txt\n",
    "    for link in links:\n",
    "        if link not in visited:\n",
    "            allowed = True\n",
    "\n",
    "            for disallowed in disallow:\n",
    "                if disallowed in link:\n",
    "                    allowed = False\n",
    "\n",
    "            if allowed:\n",
    "                crawl_r(link, stop_words, visited, depth + 1, results_file, allow, disallow)\n",
    "            else:\n",
    "                print(('\\t' * depth) + link + ' not allowed by robots.txt.')\n",
    "        else:\n",
    "            print(('\\t' * depth) + link + ' has already been visited.')\n",
    "\n",
    "\n",
    "# Get the robots.txt file from the given link\n",
    "def get_parsed_robots(url):\n",
    "    page = request.urlopen(url).read().decode('utf8')\n",
    "    allow = []\n",
    "    disallow = []\n",
    "    for line in page.split('\\n'):\n",
    "        parsed_line = line.split()\n",
    "        if parsed_line[0] == 'Allow:':\n",
    "            allow.append(parsed_line[1])\n",
    "        elif parsed_line[0] == 'Disallow:':\n",
    "            disallow.append(parsed_line[1])\n",
    "\n",
    "    return allow, disallow\n",
    "\n",
    "# Read stop words from file into set\n",
    "def read_stop_words(file_name):\n",
    "    return set(line.strip() for line in open(file_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Begin crawling at the seeded url"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "url = 'http://www.cs.utep.edu/makbar/A3/A2.html'\n",
    "stop_words = read_stop_words('stop_word.txt')\n",
    "crawl(url, stop_words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting crawl at http://www.cs.utep.edu/makbar/A3/A2.html ...\n",
      "\n",
      "Crawling http://www.cs.utep.edu/makbar/A3/A2.html ...\n",
      "\tCrawling http://www.cs.utep.edu/makbar/A3/A2/A23.html ...\n",
      "\tCrawling http://www.cs.utep.edu/makbar/A3/Ae.html ...\n",
      "\thttp://www.cs.utep.edu/makbar/A3/A4/t3.html not allowed by robots.txt.\n",
      "\t\tCrawling http://www.cs.utep.edu/makbar/A3/t2.html ...\n",
      "\t\tCrawling http://www.cs.utep.edu/makbar/A3/t1.html ...\n",
      "\tCrawling http://www.cs.utep.edu/makbar/A3/A2/A22.html ...\n",
      "\t\tCrawling http://www.cs.utep.edu/makbar/A3/A2/A31.html ...\n",
      "\t\t\tCrawling http://www.cs.utep.edu/makbar/A3/Ad.html ...\n",
      "\t\t\thttp://www.cs.utep.edu/makbar/A3/A2/A22.html has already been visited.\n",
      "\t\t\t\tCrawling http://www.cs.utep.edu/makbar/A3/E5.html ...\n",
      "\t\t\t\tCrawling http://www.cs.utep.edu/makbar/A3/E1.html ...\n",
      "\t\thttp://www.cs.utep.edu/makbar/A3/E5.html has already been visited.\n",
      "\tCrawling http://www.cs.utep.edu/makbar/A3/A2/A21.html ...\n",
      "http://www.cs.utep.edu/makbar/A3/Ad.html has already been visited.\n",
      "\tCrawling http://www.cs.utep.edu/makbar/A3/Af.html ...\n",
      "\n",
      "Saved results to macik_erik.txt\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}